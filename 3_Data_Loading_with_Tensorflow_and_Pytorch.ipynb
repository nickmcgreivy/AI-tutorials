{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962964bb-9316-4393-893f-7481aeff2400",
   "metadata": {},
   "source": [
    "# Tutorial #3: Dataloading with `tensorflow-datasets` and PyTorch dataloading\n",
    "\n",
    "We'll look at two ways to load datasets, `tensorflow-datasets` and pytorch dataloading. To illustrate, we'll use the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3459aa-ed90-4d0b-84a4-065de9dfa66e",
   "metadata": {},
   "source": [
    "### 3.1: Tensorflow datasets data loading\n",
    "\n",
    "We'll use `tensorflow-datasets`, which uses the function `tfds.load` to load datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962b2a6-32e2-4ffb-b0b2-df56e1cd426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(0)  # Set the random seed for reproducibility.\n",
    "\n",
    "train_ds: tf.data.Dataset = tfds.load('mnist', split='train', data_dir='datasets/tensorflow_datasets')\n",
    "test_ds: tf.data.Dataset = tfds.load('mnist', split='test', data_dir='datasets/tensorflow_datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc8672-c19e-4af1-b893-bab1009817f7",
   "metadata": {},
   "source": [
    "`tfds` can be understood as a high-level wrapper around the `tensorflow` API `tf.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9612f-e227-4209-906e-0e068dc4c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(train_ds, tf.data.Dataset)\n",
    "print(train_ds)\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43495d-1644-4192-b64e-64faa9f2fb38",
   "metadata": {},
   "source": [
    "Tensorflow datasets allow you to iterate over the examples in the dataset, with each example given as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487596a6-13fc-4f9f-b7aa-54cdbe45b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mnist_example in train_ds.take(1):\n",
    "    print(mnist_example.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9b68c-1442-4aff-8e20-48b063ad2db6",
   "metadata": {},
   "source": [
    "The MNIST images are handwritten digits, with labels from 0 through 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf0f0a-3e03-4720-80fd-0191cbba78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mnist_example in train_ds.take(2):\n",
    "    image = mnist_example['image']\n",
    "    label = mnist_example['label']\n",
    "    print(label)\n",
    "    print(type(image))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a91a2-058e-4298-9bde-1b681edec1cd",
   "metadata": {},
   "source": [
    "We can see the size of our training and testing datasets using the `.cardinality()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c82e0-eff3-4518-8353-e1e669240f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.cardinality().numpy())\n",
    "print(test_ds.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e856e-fbba-46ee-bb2f-7e786a8c90eb",
   "metadata": {},
   "source": [
    "Since the values of the pixels in each image range from 0 to 255, we'll normalize them to between 0 and 1 using the following lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f8f93-7c66-4d5a-a4d8-392194962373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize train set\n",
    "train_ds = train_ds.map(\n",
    "  lambda sample: {\n",
    "    'image': tf.cast(sample['image'], tf.float32) / 255,\n",
    "    'label': sample['label'],\n",
    "  }\n",
    ") \n",
    "# Normalize test set\n",
    "test_ds = test_ds.map(\n",
    "  lambda sample: {\n",
    "    'image': tf.cast(sample['image'], tf.float32) / 255,\n",
    "    'label': sample['label'],\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a0ed1-6a4f-4ca1-b32b-197b50ad0a38",
   "metadata": {},
   "source": [
    "#### Improving dataset latency with `.prefetch(1)`\n",
    "\n",
    "We can benchmark the speed of dataloading with `tfds` using `.benchmark()`. Apparently using `.prefetch(1)` improves the speed of data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7854a-4b33-45b5-ae8a-cb08c29fdd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_benchmark_example_1 = train_ds.batch(32)\n",
    "\n",
    "tfds.benchmark(ds_benchmark_example_1, batch_size=32)\n",
    "tfds.benchmark(ds_benchmark_example_1, batch_size=32)  # Second epoch faster due to auto-caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599e1ee-dffa-4081-b2c1-5b4f720bfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_benchmark_example_2 = train_ds.batch(32).prefetch(1)\n",
    "\n",
    "tfds.benchmark(ds_benchmark_example_2, batch_size=32)\n",
    "tfds.benchmark(ds_benchmark_example_2, batch_size=32)  # Second epoch faster due to auto-caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa9d9e-b143-47bb-90c2-dcf0930e7217",
   "metadata": {},
   "source": [
    "In this example, using `.prefetch(1)` doesn't seem to make much of a differrence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201ea8d-1d2d-4a32-adb8-ce9614c14c15",
   "metadata": {},
   "source": [
    "#### Data loading with `tfds` method 1: Prespecified number of training iterations \n",
    "\n",
    "We want to load our data either with a given batch size for a certain number of epochs or with a certain number of training iterations. We'll start by loading our data with a prespecified number of training iterations.\n",
    "\n",
    "We'll load the dataset with 1000 training iterations and a batch size of 32, for a total of 32000 training examples.\n",
    "\n",
    "We'll start by reloading the dataset indefinitely using the `.repeat()` function. We then shuffle the dataset using a buffer size of 1024. To explain the `.shuffle()` function, see [this](https://stackoverflow.com/questions/53514495/what-does-batch-repeat-and-shuffle-do-with-tensorflow-dataset) StackOverflow post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271baae-ad98-4541-becf-fac1e3239a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 1000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892fa9dd-a018-4bce-a1a3-55aeb342f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_1 = train_ds.repeat().shuffle(buffer_size = 1024) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5b01d-42be-4c8b-9ded-40cf7c924095",
   "metadata": {},
   "source": [
    "We can now take data indefinitely from our dataset. But we'll want to fetch data with a batch size of 32. For this, we'll use the functions `.batch(batch_size)` and the `.take(train_steps)`. We'll also use the `.prefetch(1)` function which apparently improves the latency of the dataset loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea61c4-36c5-4351-ab57-1f36cbda4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_1 = train_ds_1.batch(batch_size, drop_remainder=True)\n",
    "train_ds_1 = train_ds_1.take(train_steps)\n",
    "train_ds_1 = train_ds_1.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d3de0-6050-4be1-b8d5-49ab40d5c007",
   "metadata": {},
   "source": [
    "We can now iterate over our dataset by converting the dataset to a numpy iterator, using the `.as_numpy_iterator()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c54a3a-9909-42f5-8e49-d1fa17a627bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_ds_1.as_numpy_iterator()):\n",
    "    pass\n",
    "    \n",
    "print(step)\n",
    "print(batch['image'].shape)\n",
    "print(batch['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f79154-e7d5-4d7d-955f-1a60a640ed29",
   "metadata": {},
   "source": [
    "As you can see, we've performed 1000 iterations over the dataset and each dataset has a batch size of 32. As we wanted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70298133-9bca-4033-96ac-0f7ecfdbfe4e",
   "metadata": {},
   "source": [
    "#### Data loading with `tfds` method 2: Prespecified number of epochs\n",
    "\n",
    "We'll now perform a pre-specified number of epochs over the dataset. Instead of repeating an indefinite number of times, then batching, we'll need to batch our dataset first, then repeat `num_epochs` times. We'll also want to shuffle first, as this ensures that the batches in each epoch are different.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94a717-2a54-4cd9-be76-e58ae7364623",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c657d4e-7e37-44a0-80a2-e8163fbbbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_2 = train_ds.shuffle(buffer_size = 1024).batch(batch_size, drop_remainder=True).repeat(num_epochs).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff598e-8ccc-4888-96c3-3ea793997783",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for step, batch in enumerate(train_ds_2.as_numpy_iterator()):\n",
    "    counter += 1\n",
    "print(counter)\n",
    "print(batch['image'].shape)\n",
    "print(batch['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f9585-cc53-4447-8f3f-4c5c1d1af392",
   "metadata": {},
   "source": [
    "We've now performed 5 epochs over the dataset, as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca3439-00dc-404a-aae5-7703a50d0cb2",
   "metadata": {},
   "source": [
    "### 3.2: PyTorch data loading\n",
    "\n",
    "Loading datasets using PyTorch's `Dataloader` function is easy. We'll first import MNIST using `torchvision`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3abe04-44c6-4bbd-898c-d24807405af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887af31-0b54-44b2-b3f4-ef119a65f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c89200-4505-4cbc-924c-ec21e67cd546",
   "metadata": {},
   "source": [
    "We then convert the data into a `DataLoader` object, with a given batch size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bb56d-504f-43d8-ae44-cd8ba26fd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f7550-6e70-4b7e-8c69-6c4541fc8963",
   "metadata": {},
   "source": [
    "We can then iterate over the `DataLoader` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7049c9-5d63-4b85-b124-04c09fcc3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (train_features, train_labels) in enumerate(train_dataloader):\n",
    "    pass\n",
    "print(step)\n",
    "print(train_features.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3851f-e518-45cf-bb2a-9faf81ea557c",
   "metadata": {},
   "source": [
    "It looks like, if we want to convert these to numpy arrays, we'll have to manually convert them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
