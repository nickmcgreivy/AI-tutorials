{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da44439-dbd9-41a7-8a35-7ee050322169",
   "metadata": {},
   "source": [
    "# Tutorial \\#1: Linear Regression\n",
    "\n",
    "In this tutorial, we'll demonstrate the most basic example of statistical learning, linear regression. We'll perform linear regression in three ways: (1) analytically (2) using `scikit-learn`, and (3) using `flax`. \n",
    "\n",
    "Linear regression assumes that the data is generated from the equation $$y = w x + b + \\epsilon$$ where $w$ and $b$ are the parameters of the model and $\\epsilon$ represents some noise with an expected value of zero. The goal is to find the params $w$ and $b$ which most accurately describe future data, by using observed data. \n",
    "\n",
    "A good explanatory resource on linear regression is the lecture notes from [COS324](https://www.cs.princeton.edu/courses/archive/spring19/cos324/) at Princeton.\n",
    "* [Ordinary Least Squares Linear Regression](https://www.cs.princeton.edu/courses/archive/spring19/cos324/files/linear-regression.pdf)\n",
    "* [Maximum Likelihood Linear Regression](https://www.cs.princeton.edu/courses/archive/spring19/cos324/files/mle-regression.pdf)\n",
    "* [Least squares regression with non-linear features](https://www.cs.princeton.edu/courses/archive/spring19/cos324/files/basis-functions.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55264c3e-ee55-429c-9209-2be1bf210371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d452330-faa1-4dff-9096-d7b9e44a5ae3",
   "metadata": {},
   "source": [
    "We'll assume that our data is 1D and drawn from a distribution in which Gaussian noise is added to a non-linear ground truth distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78173f-2eed-488f-b499-b4a232a4ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(x):\n",
    "    return 3*x - 0.2*x**2 - 0.05 * x**3\n",
    "\n",
    "def generate_data(key, N_data, L):\n",
    "    key1, key2 = random.split(key)\n",
    "    x = random.uniform(key1,(N_data,)) * L\n",
    "    y = ground_truth(x) + random.normal(key2, (N_data,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b3d85-c312-4569-b61e-ade2a7903813",
   "metadata": {},
   "source": [
    "We draw a sample from the above distribution and plot the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e90c6-07d2-4b34-877a-920d5536e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ground truth and data\n",
    "\n",
    "L = 5 # domain is from 0 to 5\n",
    "N_data = 20\n",
    "x_plot = jnp.linspace(0,L,100)\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "x_data, y_data = generate_data(key, N_data=N_data, L=L)\n",
    "\n",
    "plt.plot(x_plot, ground_truth(x_plot), label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e463a56-bdaf-4926-bcd6-a2f925f042cf",
   "metadata": {},
   "source": [
    "### Method 1: Analytically calculate weights and bias\n",
    "\n",
    "Assuming the loss function $$L = ||\\boldsymbol{X}\\boldsymbol{w}-\\boldsymbol{y}||^2$$ we can derive the optimal value of $\\boldsymbol{w}$ given by $$\\boldsymbol{w} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{y}$$ We can rewrite the parameters $w$ and $b$ into a vector $\\boldsymbol{w} = [w, b]$ and append a $1$ to the data, so that our data is described by a matrix $\\boldsymbol{X} = [\\boldsymbol{x}, \\boldsymbol{1}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f93946-06ea-4b4c-820f-4cd4e4d00700",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jnp.concatenate([x_data[:,None], jnp.ones(N_data)[:,None]],axis=1)\n",
    "print(X.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392142a-a57d-4c9e-a933-074347702203",
   "metadata": {},
   "source": [
    "We can now calculate the optimal (MLE) value of $\\boldsymbol{w}$ using the above equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663994f9-8d3a-4aee-bc42-40bc8e45d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mle = jnp.linalg.inv(X.T @ X) @ X.T @ y_data\n",
    "print(w_mle)\n",
    "w, b = w_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d4cd66-05fd-4825-b491-12bf522448f1",
   "metadata": {},
   "source": [
    "Although we computed the inverse directly, note that generally it is advised to use `jax.scipy.linalg.solve()` over `jax.numpy.linalg.inv()`.\n",
    "\n",
    "We can now plot our optimal values of $w$ and $b$ compared to the ground-truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b838b00-129e-4763-9e47-181855ebe972",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plot, ground_truth(x_plot), color='blue', label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "plt.plot(x_plot, w * x_plot + b, color='green', label='Learned linear model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1858df-d8d0-4b11-90f7-c9a1b30462e6",
   "metadata": {},
   "source": [
    "### Method 2: Linear Regression with scikit-learn\n",
    "\n",
    "`scikit-learn` makes it extremely easy to fit linear regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c54748-829a-442f-b024-9951d24f17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510a0a6-795d-4f68-92f7-a7217c4da800",
   "metadata": {},
   "source": [
    "We'll first try fitting the data using 1D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fbde0-7233-4963-a4f7-771e8fc6131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    reg = LinearRegression().fit(x_data, y_data)\n",
    "except:\n",
    "    print(\"ValueError: Expected 2D array, got 1D array instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124447f-0650-406c-8f86-82437ece3aba",
   "metadata": {},
   "source": [
    "`scikit-learn` expects the input data $X$ to come in a 2D array. Let's instead pass a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d292dd-8289-41e0-8311-90d3a02016c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_data[:, None]\n",
    "reg = LinearRegression().fit(X, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8569d-d765-4dec-b5ba-0c8d0b34b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_skl, b_skl = reg.coef_, reg.intercept_\n",
    "print(w_skl, b_skl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2427f-75f3-493c-b7cd-19ad327c3fce",
   "metadata": {},
   "source": [
    "We can see that we get the same values of $w$ and $b$ as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34be3e-6669-44be-b6a1-cc9fceb08b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plot, ground_truth(x_plot), color='blue', label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "plt.plot(x_plot, w_skl[0] * x_plot + b_skl, color='green', label='Learned linear model, scikit-learn')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf6f16-4f77-482d-bbe5-071d03520c80",
   "metadata": {},
   "source": [
    "### Method 3: Using `flax` to minimize the MSE loss function using gradient descent\n",
    "\n",
    "Since the mean squared error (MSE) loss function is convex and has an analytic solution, we don't need to perform gradient descent to find a minimum. However, if we wanted to, we could also perform gradient descent to minimize the loss function. This would give the same result as computing the loss analytically.\n",
    "\n",
    "We'll use `flax` to implement and train a linear model. We will use this same code structure to train and implement more complicated neural network models in later tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc0cee-57ea-40c1-be4c-bc7e54e0bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a513c2d-ecb9-4507-9e7c-88d2365d8610",
   "metadata": {},
   "source": [
    "#### 3.1: Understanding `Rngs` in `flax`\n",
    "\n",
    "PRNG works a little differently in `flax` than in JAX. We initialize a `nnx.Rngs` object as follows: `rngs = nnx.Rngs(seed)` where `seed` is an `int`.\n",
    "\n",
    "Look at the following code below to see how `nnx.Rngs` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb27c3-1e40-4942-a292-30a0e43de8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0) # seed is 0 for all streams\n",
    "print(\"The first time rngs is called, it gives a key\")\n",
    "print(rngs.params())\n",
    "print(\"The key automatically changes each time rngs is called\")\n",
    "print(rngs.dropout())\n",
    "print(\"I can call whatever stream I want\")\n",
    "print(rngs.random_stream_whatever_I_want())\n",
    "\n",
    "# Different streams can have different keys\n",
    "rngs = nnx.Rngs(0, params=1) # seed is 0 for all streams except for params stream\n",
    "print(\"Params stream has seed of 1, has different key\")\n",
    "print(rngs.params()) # print params stream\n",
    "print(\"Other streams have seeds of 2, have same keys as before\")\n",
    "print(rngs.dropout()) # print dropout stream\n",
    "print(rngs.random_stream_whatever_I_want())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1fbd8-b09e-4aba-b966-b754095dcd83",
   "metadata": {},
   "source": [
    "#### 3.2: Create linear model in `flax`\n",
    "\n",
    "We'll now create a subclass of `nnx.Module` which represents our linear regression model. We need to implement `__init__` and `__call__` methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63486871-2dda-421d-86e7-95d6b4e27d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nnx.Module):\n",
    "    def __init__(self, din: int, rngs: nnx.Rngs):\n",
    "        key = rngs.params()\n",
    "        self.w = nnx.Param(random.normal(key, (din,)))\n",
    "        self.b = nnx.Param(0.0)\n",
    "\n",
    "    def __call__(self, x: jax.Array):\n",
    "        return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0a256-4e08-4cd5-8758-dc953dcc478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(1, rngs = nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.sgd(1e-3))\n",
    "y = model(jnp.asarray([1.0]))\n",
    "print(y)\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce421d8-a845-4d97-b907-1c6785a02a7f",
   "metadata": {},
   "source": [
    "#### 3.3: Write loss function and train model\n",
    "\n",
    "Our training step takes advantage of the fact that `nnx.Module` classes are mutable, meaning that the params are stored and updated within the class. By calling `optimizer.update(grads)`, the parameters are updated automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea337e3-14b5-4057-9075-6d31a6da3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, y):\n",
    "    def loss_fn(model: LinearRegression):\n",
    "        y_pred = nnx.vmap(model)(x)\n",
    "        return jnp.mean((y - y_pred)**2)\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782e0fa-8f21-466d-abf7-077b7f4ce034",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_data.reshape(N_data, 1)\n",
    "Y = y_data.reshape(N_data)\n",
    "\n",
    "print(nnx.vmap(model)(X).shape)\n",
    "print(Y.shape)\n",
    "try:\n",
    "    loss = train_step(model, optimizer, X, y_data)\n",
    "    print(loss)\n",
    "except:\n",
    "    print(\"shape of x_data and y_data incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3d900-a2bd-42b2-97b0-16351d2aca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 10000\n",
    "losses = []\n",
    "for _ in range(N_train):\n",
    "    loss = train_step(model, optimizer, X, Y)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99533d8-ef76-446f-9148-fd86ed4f006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafd6e6-f07b-4d86-add0-a201b4429ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.w, model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f809ed-0e4e-4b73-a87e-c2cdd6515027",
   "metadata": {},
   "source": [
    "The values of $w$ and $b$ computed using gradient descent are almost identical to the analytically computed $w$ and $b$ from methods #1 and #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb002139-2c14-4c21-b845-70417898d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plot, ground_truth(x_plot), color='blue', label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "plt.plot(x_plot, model.w[0] * x_plot + model.b, color='green', label='Learned linear model, gradient descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
