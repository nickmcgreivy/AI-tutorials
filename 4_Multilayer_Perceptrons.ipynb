{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fce470-f485-48c4-b666-f0afdd1456cc",
   "metadata": {},
   "source": [
    "# Tutorial #4: Multilayer Perceptrons\n",
    "\n",
    "Multilayer perceptrons (MLPs), also known as feedforward neural networks, apply a series of linear transformations followed by non-linear activation functions. At each layer in an MLP, the hidden units (neurons) $\\boldsymbol{h}$ are given by $$\\boldsymbol{h} = g(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b})$$ where $\\boldsymbol{x}$ is the input from the previous hidden layer and $g$ is a non-linear function. MLPs can be used either in regression, where the output layer is linear layer, or in classification, where the output layer is a softmax. \n",
    "\n",
    "A good resource on MLPs is Chapter 6 of the [Deep Learning Book](https://www.deeplearningbook.org/).\n",
    "\n",
    "We'll start by replacing the linear regression model on the synthetic dataset from tutorial #1 with an MLP. We'll then train an MLP to perform classification on the MNIST dataset of handwritten digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de67e09-cf0b-4350-9eaa-09820acabd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25127d96-ee89-493e-b17e-9bb680e63b99",
   "metadata": {},
   "source": [
    "### 3.1: MLP for 1D regression on a synthetic dataset\n",
    "\n",
    "We'll begin by recreating the data used in tutorial #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65585aca-0932-40b5-9a66-d691378dc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(x):\n",
    "    return 3*x - 0.2*x**2 - 0.05 * x**3\n",
    "\n",
    "def generate_data(key, N_data, L):\n",
    "    key1, key2 = random.split(key)\n",
    "    x = random.uniform(key1,(N_data,)) * L\n",
    "    y = ground_truth(x) + random.normal(key2, (N_data,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d10728-4a5b-4f2b-9a4c-c7788bc1f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ground truth and data\n",
    "\n",
    "L = 5 # domain is from 0 to 5\n",
    "N_data = 20\n",
    "N_plot = 100\n",
    "x_plot = jnp.linspace(0,L,N_plot)\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "x_data, y_data = generate_data(key, N_data=N_data, L=L)\n",
    "\n",
    "plt.plot(x_plot, ground_truth(x_plot), label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b48d4-995e-4cbe-957c-5c830f6b3074",
   "metadata": {},
   "source": [
    "We'll create an MLP with one input variable $x$, three hidden layers with five hidden units each, ReLU activation functions, and one output variable $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032c5c8-9381-4bfd-b556-202903af03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import flax\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a09d0-613a-49c2-bf85-582d31be5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scalarMLP(nnx.Module):\n",
    "    def __init__(self, dhiddens: list[int], rngs: nnx.Rngs):\n",
    "        self.linear_in = nnx.Linear(1, dhiddens[0], rngs=rngs)\n",
    "        self.layers = []\n",
    "        for j in range(len(dhiddens)-1):\n",
    "            self.layers.append(nnx.Linear(dhiddens[j], dhiddens[j+1], rngs=rngs))\n",
    "        self.linear_out = nnx.Linear(dhiddens[-1], 1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.linear_in(x))\n",
    "        for layer in self.layers:\n",
    "            x = nnx.relu(layer(x))\n",
    "        return self.linear_out(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2665a1-2c9c-4c5a-b797-14ffd4358810",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "model = scalarMLP([5, 5, 5], rngs)\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63786c8f-66d5-4fd6-8968-b1ffd7e72c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, y):\n",
    "    def loss_fn(model):\n",
    "        y_pred = nnx.vmap(model)(x)\n",
    "        return jnp.mean((y - y_pred)**2)\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ba4a6-298b-414f-87b8-1b1d5a1fdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_data.reshape(N_data, 1)\n",
    "Y = y_data.reshape(N_data)\n",
    "\n",
    "try:\n",
    "    loss = train_step(model, optimizer, X, y_data)\n",
    "    print(loss)\n",
    "except:\n",
    "    print(\"shape of x_data and y_data incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b982dfe8-3be4-4680-969b-33e90848569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 20000\n",
    "losses = []\n",
    "for _ in range(N_train):\n",
    "    loss = train_step(model, optimizer, X, Y)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526011e-5f21-4afb-9f3f-f18977bc4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb37ef-7415-4e47-b9cf-c1f96332afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plot, ground_truth(x_plot), color='blue', label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "plt.plot(x_plot, nnx.vmap(model)(x_plot.reshape(N_plot,1)), color='green', label='Trained MLP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61982897-d2b1-4e08-a4c5-49d3507ca569",
   "metadata": {},
   "source": [
    "#### 3.1.1: Varying the complexity of MLPs\n",
    "\n",
    "We saw that our MLP with three hidden layers and five hidden units gives a piecewise linear function. What happens as we change the number of hidden units and number of layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea3b26-6cb2-44b8-90b7-11822cb43b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 20000\n",
    "rngs = nnx.Rngs(0)\n",
    "hdims_list = [[3,3],[5,5,5],[7,7,7,7],[9,9,9,9,9]]\n",
    "models = []\n",
    "optimizers = []\n",
    "\n",
    "for hdims in hdims_list:\n",
    "    model = scalarMLP(hdims, rngs)\n",
    "    models.append(model)\n",
    "    optimizers.append(nnx.Optimizer(model, optax.adam(1e-3)))\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    optimizer = optimizers[j]\n",
    "    print(j)\n",
    "    for _ in range(N_train):\n",
    "        loss = train_step(model, optimizer, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e3205-3ffb-4b02-94d3-f3d3a600a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for hdims in hdims_list:\n",
    "    labels.append(\"MLP: {} units, {} layers\".format(hdims[0], len(hdims)))\n",
    "\n",
    "plt.plot(x_plot, ground_truth(x_plot), color='blue', label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "for j, model in enumerate(models):\n",
    "    plt.plot(x_plot, nnx.vmap(model)(x_plot.reshape(N_plot,1)), label=labels[j])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f69b4-2731-4b72-82b7-7e74302ec896",
   "metadata": {},
   "source": [
    "As we can see, the deeper networks with more hidden units result in more complex functions than the smaller networks. The MLP with 3 hidden units and 2 layers simply results in a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d7cc9-b588-48a5-977b-5374952757a8",
   "metadata": {},
   "source": [
    "### 3.2: MLP for MNIST\n",
    "\n",
    "First we need to load the MNIST dataset. We'll use `tfds` and prepare the dataset for `num_epochs` training epochs with a batch size of 32."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
