{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fce470-f485-48c4-b666-f0afdd1456cc",
   "metadata": {},
   "source": [
    "# Tutorial #4: Multilayer Perceptrons\n",
    "\n",
    "Multilayer perceptrons (MLPs), also known as feedforward neural networks, apply a series of linear transformations followed by non-linear activation functions. At each layer in an MLP, the hidden units (neurons) $\\boldsymbol{h}$ are given by $$\\boldsymbol{h} = g(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b})$$ where $\\boldsymbol{x}$ is the input from the previous hidden layer and $g$ is a non-linear function. MLPs can be used either in regression, where the output layer is linear layer, or in classification, where the output layer is a softmax. \n",
    "\n",
    "A good resource on MLPs is Chapter 6 of the [Deep Learning Book](https://www.deeplearningbook.org/).\n",
    "\n",
    "We'll start by replacing the linear regression model on the synthetic dataset from tutorial #1 with an MLP. We'll then train an MLP to perform classification on the MNIST dataset of handwritten digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de67e09-cf0b-4350-9eaa-09820acabd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.random as random\n",
    "from flax import nnx\n",
    "import flax\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25127d96-ee89-493e-b17e-9bb680e63b99",
   "metadata": {},
   "source": [
    "### 3.1: MLP for 1D regression on a synthetic dataset\n",
    "\n",
    "We'll begin by recreating the data used in tutorial #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65585aca-0932-40b5-9a66-d691378dc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(x):\n",
    "    return 3*x - 0.2*x**2 - 0.05 * x**3\n",
    "\n",
    "def generate_data(key, N_data, L):\n",
    "    key1, key2 = random.split(key)\n",
    "    x = random.uniform(key1,(N_data,)) * L\n",
    "    y = ground_truth(x) + random.normal(key2, (N_data,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d10728-4a5b-4f2b-9a4c-c7788bc1f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ground truth and data\n",
    "\n",
    "L = 5 # domain is from 0 to 5\n",
    "N_data = 20\n",
    "N_plot = 100\n",
    "x_plot = jnp.linspace(0,L,N_plot)\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "x_data, y_data = generate_data(key, N_data=N_data, L=L)\n",
    "\n",
    "plt.plot(x_plot, ground_truth(x_plot), label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b48d4-995e-4cbe-957c-5c830f6b3074",
   "metadata": {},
   "source": [
    "We'll create an MLP with one input variable $x$, three hidden layers with five hidden units each, ReLU activation functions, and one output variable $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a09d0-613a-49c2-bf85-582d31be5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scalarMLP(nnx.Module):\n",
    "    def __init__(self, dhiddens: list[int], rngs: nnx.Rngs):\n",
    "        self.linear_in = nnx.Linear(1, dhiddens[0], rngs=rngs)\n",
    "        self.layers = []\n",
    "        for j in range(len(dhiddens)-1):\n",
    "            self.layers.append(nnx.Linear(dhiddens[j], dhiddens[j+1], rngs=rngs))\n",
    "        self.linear_out = nnx.Linear(dhiddens[-1], 1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.linear_in(x))\n",
    "        for layer in self.layers:\n",
    "            x = nnx.relu(layer(x))\n",
    "        return self.linear_out(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2665a1-2c9c-4c5a-b797-14ffd4358810",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "model = scalarMLP([5, 5, 5], rngs)\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63786c8f-66d5-4fd6-8968-b1ffd7e72c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, y):\n",
    "    def loss_fn(model):\n",
    "        y_pred = nnx.vmap(model)(x)\n",
    "        return jnp.mean((y - y_pred)**2)\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ba4a6-298b-414f-87b8-1b1d5a1fdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_data.reshape(N_data, 1)\n",
    "Y = y_data.reshape(N_data)\n",
    "\n",
    "try:\n",
    "    loss = train_step(model, optimizer, X, y_data)\n",
    "    print(loss)\n",
    "except:\n",
    "    print(\"shape of x_data and y_data incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b982dfe8-3be4-4680-969b-33e90848569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 20000\n",
    "losses = []\n",
    "for _ in range(N_train):\n",
    "    loss = train_step(model, optimizer, X, Y)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526011e-5f21-4afb-9f3f-f18977bc4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb37ef-7415-4e47-b9cf-c1f96332afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plot, ground_truth(x_plot), color='blue', label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "plt.plot(x_plot, nnx.vmap(model)(x_plot.reshape(N_plot,1)), color='green', label='Trained MLP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61982897-d2b1-4e08-a4c5-49d3507ca569",
   "metadata": {},
   "source": [
    "#### 3.1.1: Varying the complexity of MLPs\n",
    "\n",
    "We saw that our MLP with three hidden layers and five hidden units gives a piecewise linear function. What happens as we change the number of hidden units and number of layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea3b26-6cb2-44b8-90b7-11822cb43b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 20000\n",
    "rngs = nnx.Rngs(0)\n",
    "hdims_list = [[3,3],[5,5,5],[7,7,7,7],[9,9,9,9,9]]\n",
    "models = []\n",
    "optimizers = []\n",
    "\n",
    "for hdims in hdims_list:\n",
    "    model = scalarMLP(hdims, rngs)\n",
    "    models.append(model)\n",
    "    optimizers.append(nnx.Optimizer(model, optax.adam(1e-3)))\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    optimizer = optimizers[j]\n",
    "    print(j)\n",
    "    for _ in range(N_train):\n",
    "        loss = train_step(model, optimizer, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e3205-3ffb-4b02-94d3-f3d3a600a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for hdims in hdims_list:\n",
    "    labels.append(\"MLP: {} units, {} layers\".format(hdims[0], len(hdims)))\n",
    "\n",
    "plt.plot(x_plot, ground_truth(x_plot), color='blue', label='ground truth')\n",
    "plt.scatter(x_data, y_data, color='red', marker='x', label='data')\n",
    "for j, model in enumerate(models):\n",
    "    plt.plot(x_plot, nnx.vmap(model)(x_plot.reshape(N_plot,1)), label=labels[j])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f69b4-2731-4b72-82b7-7e74302ec896",
   "metadata": {},
   "source": [
    "As we can see, the deeper networks with more hidden units result in more complex functions than the smaller networks. The MLP with 3 hidden units and 2 layers simply results in a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d7cc9-b588-48a5-977b-5374952757a8",
   "metadata": {},
   "source": [
    "### 3.2: MLP for MNIST\n",
    "\n",
    "This part of the tutorial is heavily influenced by the `flax` [MNIST tutorial](https://flax.readthedocs.io/en/latest/mnist_tutorial.html), in which a convolutional neural network is trained using MNIST.\n",
    "\n",
    "#### 3.2.1: Load MNIST dataset\n",
    "\n",
    "First we need to load the MNIST dataset. We'll use `tfds` and prepare the dataset for `num_epochs` training epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d243a-bfdd-4103-8c0d-26933ed35d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(0)  # Set the random seed for reproducibility.\n",
    "\n",
    "train_ds: tf.data.Dataset = tfds.load('mnist', split='train', data_dir='datasets/tensorflow_datasets')\n",
    "test_ds: tf.data.Dataset = tfds.load('mnist', split='test', data_dir='datasets/tensorflow_datasets')\n",
    "\n",
    "# normalize train set\n",
    "train_ds = train_ds.map(\n",
    "  lambda sample: {\n",
    "    'image': tf.cast(sample['image'], tf.float32) / 255,\n",
    "    'label': sample['label'],\n",
    "  }\n",
    ") \n",
    "# Normalize test set\n",
    "test_ds = test_ds.map(\n",
    "  lambda sample: {\n",
    "    'image': tf.cast(sample['image'], tf.float32) / 255,\n",
    "    'label': sample['label'],\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688df4ee-725c-4b8b-bede-66bcd3c41c94",
   "metadata": {},
   "source": [
    "We'll use a batch size of 32, and train for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69bb97-70a2-486d-8049-886284f0974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "buffer_size = 1024\n",
    "lr = 1e-3\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e89a60-559e-4d78-b63a-4607a3e87ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_iterator = train_ds.shuffle(buffer_size = 1024).batch(batch_size, drop_remainder=True).repeat(num_epochs).prefetch(1)\n",
    "test_ds_iterator = test_ds.batch(batch_size, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c457f7f-8e91-4e43-8c96-4d48171949bb",
   "metadata": {},
   "source": [
    "#### 3.2.2: Create MLP in `flax`\n",
    "\n",
    "Next we'll create our MLP model and our optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084312ff-abcb-4d82-ac3c-ab150be0bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLP(nnx.Module):\n",
    "    def __init__(self, din: int, dhiddens: list[int], dout: int, rngs: nnx.Rngs):\n",
    "        self.linear_in = nnx.Linear(din, dhiddens[0], rngs=rngs)\n",
    "        self.layers = []\n",
    "        for j in range(len(dhiddens)-1):\n",
    "            self.layers.append(nnx.Linear(dhiddens[j], dhiddens[j+1], rngs=rngs))\n",
    "        self.linear_out = nnx.Linear(dhiddens[-1], dout, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.flatten()\n",
    "        x = nnx.relu(self.linear_in(x))\n",
    "        for layer in self.layers:\n",
    "            x = nnx.relu(layer(x))\n",
    "        return self.linear_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed623eb-a501-4566-9c56-145f250b9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_ds.take(1):\n",
    "    MNIST_image = jnp.asarray(sample['image'])\n",
    "    MNIST_output = sample['label']\n",
    "MNIST_image_dims = MNIST_image.shape\n",
    "print(MNIST_image_dims)\n",
    "print(MNIST_image.flatten().shape[0])\n",
    "print(MNIST_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f2fea-f2f9-41bd-899b-759a2fa63bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "model = MNIST_MLP(MNIST_image.flatten().shape[0], [64, 64, 64], 10, rngs)\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(lr, momentum))\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52bd05-74bc-4beb-91dc-ba061e2b2e4c",
   "metadata": {},
   "source": [
    "We can calculate the number of parameters of our MLP model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d082334-bd5e-423a-971b-e2ebd01c4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphdef, params = nnx.split(model, nnx.Param)\n",
    "print(sum(x.size for x in jax.tree_leaves(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3fae7-34e9-42f1-b3f3-ec51becf6465",
   "metadata": {},
   "source": [
    "Now we'll test our model on one sample from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66d8ed-1eb8-4a05-9678-bf8bf64231f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model(MNIST_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6deb07-ed7d-489f-82e9-4b345031ac19",
   "metadata": {},
   "source": [
    "#### 3.2.3: Write train and eval steps\n",
    "\n",
    "Before we train our model, we'll set up some functions with which to track the performance of the MLP. We'll want to measure the loss and accuracy on both the training set and the testing set. To do this, we'll introduce a `nnx.MultiMetric` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96413c4a-faa2-42ff-b8b6-262a1b8758a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = nnx.MultiMetric(\n",
    "  accuracy=nnx.metrics.Accuracy(),\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a26d0e-0887-49ac-ae25-c04a882e620b",
   "metadata": {},
   "source": [
    "We'll introduce a loss function and a `train_step` function, as usual. We could have converted our `batch['label']` into a one-hot encoding and used `optax.softmax_cross_entropy`, but instead we'll use `optax.softmax_cross_entropy_with_integer_labels`. We'll output both the classification logits and the loss value, because the logits will be useful in computing accuracy statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575b771-8501-40bd-9baa-a04b87ca11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model: MNIST_MLP, batch):\n",
    "    logits = nnx.vmap(model)(batch['image'])\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, batch['label']))\n",
    "    return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MNIST_MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux = True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss = loss, logits=logits, labels=batch['label'])\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6c27b-547b-4475-9063-6358c7b4aeb1",
   "metadata": {},
   "source": [
    "The only difference in the `train_step` between the MLP we wrote in 3.1 and the MNIST_MLP we are writing here is in the `metrics.update` function, which computes the average value of the loss and the accuracy. The `nnx.MultiMetric` objects must be smart enough to calculate the accuracy from the logits and the labels.\n",
    "\n",
    "We'll also write an `eval_step`, which we'll use to compute metrics on the test set over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481efd82-7d33-4e3a-b5c7-3645ee4a6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def eval_step(model: MNIST_MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch['label'])  # In-place updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be912b19-5ebd-4f90-9864-9f21d4c18dfc",
   "metadata": {},
   "source": [
    "#### 3.2.4: Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc53502-e73c-4d2f-8407-d6bef79a8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "eval_every = 200\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'train_accuracy': [],\n",
    "  'test_loss': [],\n",
    "  'test_accuracy': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5495a-a615-4d11-a05f-4ebe4e47f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_ds_iterator.as_numpy_iterator()):\n",
    "    train_step(model, optimizer, metrics, batch)\n",
    "    \n",
    "    if step % eval_every == 0:\n",
    "\n",
    "        # log training metrics to history\n",
    "        for metric, value in metrics.compute().items():\n",
    "            metrics_history[f'train_{metric}'].append(value)\n",
    "\n",
    "        # reset the metrics between each eval_every number of batches\n",
    "        # also to compute metrics on the test set\n",
    "        metrics.reset() \n",
    "\n",
    "        # loop through test set to compute metrics\n",
    "        for test_batch in test_ds_iterator.as_numpy_iterator():\n",
    "            eval_step(model, metrics, test_batch)\n",
    "\n",
    "        # log testing metrics to history\n",
    "        for metric, value in metrics.compute().items():\n",
    "            metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "        # reset metrics for next set of training runs\n",
    "        metrics.reset()\n",
    "\n",
    "        # plot metrics over time\n",
    "        clear_output(wait=True)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax1.set_title('Loss')\n",
    "        ax2.set_title('Accuracy')\n",
    "        for dataset in ('train', 'test'):\n",
    "          ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n",
    "          ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\n",
    "        ax1.legend()\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4292546-0edb-4615-865e-d4a350e5b558",
   "metadata": {},
   "source": [
    "#### 3.2.5: Evaluate MLP on Test Set\n",
    "\n",
    "We'll first print the accuracy on the testing set. Then we'll compare our predictions to the images to visually confirm that our model gives accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40395725-019c-4cf6-aacc-85057654b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear metrics\n",
    "metrics.reset()\n",
    "\n",
    "# set model to eval mode, which is unnecessary for our MLP but is required when dropout and batchnorm are used (see future tutorials)\n",
    "model.eval()\n",
    "\n",
    "for test_batch in test_ds_iterator.as_numpy_iterator():\n",
    "    eval_step(model, metrics, test_batch)\n",
    "\n",
    "print(metrics.compute()['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76163df0-df4e-4797-b0de-77b1af7cddb0",
   "metadata": {},
   "source": [
    "We get an accuracy of 97% on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35d92e-a615-4706-9bdf-3c6d881feafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def pred_step(model: MNIST_MLP, batch):\n",
    "    logits = nnx.vmap(model)(batch['image'])\n",
    "    return logits.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c7cc9-eda4-410d-820d-b576eb55ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_iterator = test_ds.batch(25).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a5844-ea32-4433-ac29-920b9ea89bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test_ds_iterator.next()\n",
    "\n",
    "preds = pred_step(model, test_batch)\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(test_batch['image'][i], cmap='gray')\n",
    "    ax.set_title(f'predicted label = {preds[i]}\\ntrue label = {test_batch['label'][i]}')\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
