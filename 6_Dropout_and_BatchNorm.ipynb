{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474a95e3-894a-40f2-a937-808275196aa5",
   "metadata": {},
   "source": [
    "# Tutorial #6: Dropout and BatchNorm\n",
    "\n",
    "### 6.1: Dropout\n",
    "\n",
    "Dropout is a method of regularizing networks. Dropout is an extremely simple: during training, multiply the input to each unit by either 0 or 1, multiplying by 1 with probability $(1-p)$ and by 0 with probability $p$. Typically, $p=0.5$ is chosen for hidden units, as it empirically tends to work well in applications.\n",
    "\n",
    "During testing, none of the units are multiplied by zero. Empiricially, it has been shown that dropout has better performance if the expected value of the output of each hidden unit matches the distribution seen during training. Since $1/(1-p)$ times more hidden units are included in each linear transformation during testing compared to training, the expected value of each output unit will be multiplied by $1/(1-p)$. To counteract this multiplication in expected value, the weights of the linear transformation are simply multiplied by $(1-p)$ during testing.\n",
    "\n",
    "A simpler option (called inverted dropout) is to do nothing differently during testing, but during training to multiply the hidden units by $1/(1-p)$ in addition to setting them to zero with probability $p$. This keeps the distribution of hidden units comparable between testing and training.\n",
    "\n",
    "For a layer within a neural network with input $\\boldsymbol{x}$, non-linear activation function $g(\\boldsymbol{z})$, and linear transformation matrix $\\boldsymbol{W}$, and bias $\\boldsymbol{b}$, the output $\\boldsymbol{h}$ of the hidden layer is given by $$\\boldsymbol{h} = g(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}).$$ Dropout performs elementwise multiplication of $\\boldsymbol{x}$ with a binary mask $\\boldsymbol{r}$ where each element of $\\boldsymbol{r}$ is drawn from a Bernoulli distribution with probability $1-p$. During training, the dropout layer becomes (assuming inverted dropout) $$r_j \\sim \\frac{1}{1-p}\\textnormal{Bernoulli}(1-p)$$ $$ \\boldsymbol{\\tilde{x}} = \\boldsymbol{r} * \\boldsymbol{x}$$ $$\\boldsymbol{h} = g(\\boldsymbol{W} \\boldsymbol{\\tilde{x}} + \\boldsymbol{b}).$$ During testing, the layer is still $$\\boldsymbol{h} = g(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}).$$\n",
    "\n",
    "Dropout tends to take a different form for convolutional networks, recurrent networks, and attention layers than in MLPs. However, the basic idea of probabalistically setting hidden units to zero still remains.\n",
    "\n",
    "Good resources for understanding dropout include the original 2014 paper by [Srivastava et al.](TODO), section 7.12 of the [Deep Learning Book](TODO), and [this blog post](https://medium.com/biased-algorithms/the-role-of-dropout-in-neural-networks-fffbaa77eee7#:~:text=Dropout%20has%20been%20successfully%20integrated,layers%20after%20the%20convolutional%20blocks.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc3912-9265-4db4-abf2-2c4c004674fe",
   "metadata": {},
   "source": [
    "### 6.1.1: Dropout in `flax`\n",
    "\n",
    "Dropout in `flax` is extremely easy to implement. We simply create a `nnx.Dropout(p, rngs)` layer, and call it to mask the input to the each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca78324-4d2b-41a7-be66-a1206cbe2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import jax\n",
    "from flax import nnx\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a782c7c-6938-4a4d-8ec5-fd4571be66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutTestMLP(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.linear1 = nnx.Linear(4, 16, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(16, 10, rngs=rngs)\n",
    "        self.dropout = nnx.Dropout(0.5, rngs = rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.linear1(x))\n",
    "        return self.linear2(self.dropout(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e9a6e-8768-4af1-a8ce-88b0e6670d23",
   "metadata": {},
   "source": [
    "In flax, we have to call `model.train()` to set the dropout layers to mask the inputs, and `model.eval()` to multiply the weights by $1/p$ for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159decb5-bcc6-4c01-bb48-1b2b4ad8e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train dropout, mask chosen randomly\n",
      "[-0.24484156  1.3231333  -3.9299839  -3.0473146   3.225388    0.22339313\n",
      "  0.5151764  -1.9571856   0.47121215  0.05025537]\n",
      "Second train dropout, mask chosen randomly with different RNG key\n",
      "[-1.6144576   0.61325073 -3.1674788  -2.2446618   2.230607   -0.08878337\n",
      "  1.1725848   0.04968039  0.73547506  2.1024349 ]\n",
      "First eval dropout, no mask\n",
      "[-0.87284786  0.84970397 -1.2577161  -0.75616425  0.4537591  -0.6594648\n",
      "  0.9112347   0.19184317  0.8342736   1.534824  ]\n",
      "Second eval dropout, deterministic output same as first eval\n",
      "[-0.87284786  0.84970397 -1.2577161  -0.75616425  0.4537591  -0.6594648\n",
      "  0.9112347   0.19184317  0.8342736   1.534824  ]\n"
     ]
    }
   ],
   "source": [
    "model = DropoutTestMLP(nnx.Rngs(0))\n",
    "\n",
    "model.train()\n",
    "\n",
    "test_input = jax.random.normal(jax.random.PRNGKey(0), (4,))\n",
    "print(\"First train dropout, mask chosen randomly\")\n",
    "print(model(test_input))\n",
    "print(\"Second train dropout, mask chosen randomly with different RNG key\")\n",
    "print(model(test_input))\n",
    "\n",
    "model = DropoutTestMLP(nnx.Rngs(0))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"First eval dropout, no mask\")\n",
    "print(model(test_input))\n",
    "print(\"Second eval dropout, deterministic output same as first eval\")\n",
    "print(model(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce946be-5114-4d62-b47e-c8304f1bc82c",
   "metadata": {},
   "source": [
    "When using Dropout layers, we have to remember to call these functions before training and testing, and when evaluating performance on a development set or testing set.\n",
    "\n",
    "#### 6.1.2: Understanding `nnx.Dropout`\n",
    "\n",
    "Let's create a dropout layer and display it using `nnx.display()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5889a55-a55c-436f-badd-6e26381d1b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(rate=0.2, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(\n",
      "  default=RngStream(\n",
      "    key=RngKey(\n",
      "      value=Array((), dtype=key<fry>) overlaying:\n",
      "      [0 0],\n",
      "      tag='default'\n",
      "    ),\n",
      "    count=RngCount(\n",
      "      value=Array(0, dtype=uint32),\n",
      "      tag='default'\n",
      "    )\n",
      "  )\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "layer = nnx.Dropout(0.2, rngs=nnx.Rngs(0))\n",
    "nnx.display(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d006d-38b4-4f42-92f0-c7cb00d1c19e",
   "metadata": {},
   "source": [
    "There are a couple of things to note here. First, the dropout layer takes as input the keyword argument `broadcast_dims`, which allows for an entire dimension of activations to be dropped, rather than individual activations to be dropped at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263525a2-78e0-4d47-b89b-45bf91d92b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.        ]\n",
      " [-0.8671889  -0.1572347 ]\n",
      " [ 0.3521818  -1.9441785 ]\n",
      " [-0.9905975   0.9887572 ]\n",
      " [ 1.3286986  -1.900327  ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.5541694   3.0209296 ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.04940141 -3.8329544 ]]\n"
     ]
    }
   ],
   "source": [
    "layer = nnx.Dropout(0.5, broadcast_dims=(1,), rngs=nnx.Rngs(0))\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(key, (10,2))\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0890a-ec71-4fa7-8ec6-31ac94fb8495",
   "metadata": {},
   "source": [
    "This is useful when using dropout with convolutional layers, as dropout with convolutional layers will often choose to drop out entire feature maps. For example, applying a convolutional layer to a 2D image with C channels of size H x W x C will result in a matrix of size H x W x D, where D is the number of output filters; dropout sets to zero all of the activations in each of the D filters at random with probability $p$.\n",
    "\n",
    "Second, the probability is the probability of dropping the activation and NOT the probability of keeping the activation. In the below example, we choose $p=0.9$ and see that almost all of the activations are dropped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ade8ca-24fb-4014-9123-f32d813832db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.       21.302025]\n",
      "[-24.424557 -20.356806   0.         0.         0.         0.\n",
      "   0.         0.       -13.105359   0.      ]\n",
      "[ 0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.       21.302025]\n"
     ]
    }
   ],
   "source": [
    "layer = nnx.Dropout(0.9, rngs=nnx.Rngs(0))\n",
    "key, subkey = jax.random.split(key)\n",
    "x = jax.random.normal(subkey, (10,))\n",
    "print(layer(x))\n",
    "print(layer(x))\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2f0f8-ff88-4be4-9137-b9d411d38447",
   "metadata": {},
   "source": [
    "Third, by default dropout layers are set to be in training mode (`deterministic=False`) rather than evaluation mode (`deterministic=True`). But by calling `model.eval()`, we set `deterministic=True`. In eval mode, none of the activations are dropped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c48220-b05f-4cb1-a036-986a3d9f9865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[ 0.         -0.8032088   0.          1.7567555  -1.7235099   0.\n",
      "  0.         -0.24468681  0.          0.14304025]\n",
      "True\n",
      "[-1.2574776  -0.4016044  -1.1213601   0.87837774 -0.86175495  0.34651348\n",
      "  0.9404431  -0.12234341 -1.1891836   0.07152013]\n"
     ]
    }
   ],
   "source": [
    "layer = nnx.Dropout(0.5, rngs=nnx.Rngs(1))\n",
    "key, subkey = jax.random.split(key)\n",
    "x = jax.random.normal(subkey, (10,))\n",
    "\n",
    "print(layer.deterministic)\n",
    "print(layer(x))\n",
    "\n",
    "layer.eval()\n",
    "print(layer.deterministic)\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f15f6-1cf4-445a-a9ce-77a47d3f09aa",
   "metadata": {},
   "source": [
    "Fourth, in training mode, the non-zero activations are multiplied by $1/(1-p)$, while in testing mode the activations are not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2232a1-4f9b-4bb6-956d-3e4b6574e74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[ 0.        0.        0.        0.       -8.629354  0.        0.\n",
      "  0.        0.        0.      ]\n",
      "True\n",
      "[-1.3877681   0.77485436  1.5404932   1.8419101  -0.86293536 -0.8070163\n",
      " -0.2005241   0.7834719  -0.9859735   0.26376608]\n"
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "x = jax.random.normal(subkey, (10,))\n",
    "\n",
    "layer = nnx.Dropout(0.9, rngs=nnx.Rngs(1))\n",
    "layer.train()\n",
    "print(layer.deterministic)\n",
    "print(layer(x))\n",
    "\n",
    "layer = nnx.Dropout(0.9, rngs=nnx.Rngs(1))\n",
    "layer.eval()\n",
    "print(layer.deterministic)\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9b5d0-cb5d-4a1a-99cf-540a5c47b0c1",
   "metadata": {},
   "source": [
    "### 6.2: BatchNorm\n",
    "\n",
    "BatchNorm is a way of improving the training process of neural networks with gradient descent. BatchNorm can be used simultaneously with Dropout, or it can be used without Dropout. \n",
    "\n",
    "BatchNorm is performed in two steps. \n",
    "\n",
    "First, for each activation, normalize the activation so that it has mean zero and variance 1. Instead of calculating the mean and variance over the entire dataset, during training BatchNorm uses the statistics from the current batch to estimate a mean and variance. So, for the $k$th activation value $x^{k}$ in a BatchNorm layer, during training the activation is normalized with the equation $$\\hat{x}^{k} = \\frac{x^k - \\mathbb{E}[x^k]}{\\sqrt{\\textnormal{Var}[x^k]}}$$  where the expectation and variance are calculated by summing over the batch using $$\\mathbb{E}[x^k] = \\frac{1}{m}\\sum_{i=1}^n x^k_i$$ $$\\textnormal{Var}[x^k] = \\frac{1}{m}\\sum_{i=1}^m (x^k_i - \\mathbb{E}[x^k])^2 + \\delta$$ where $\\delta$ is a small number to prevent numerical underflow. During testing, the means and variances are calculated using stored running averages from the training $$\\mathbb{E}[x^k]_{\\textnormal{running}} = \\alpha \\mathbb{E}[x^k]_{\\textnormal{running}} + (1-\\alpha) \\mathbb{E}[x^k]_{\\textnormal{batch}}$$ $$\\sqrt{\\textnormal{Var}[x^k]_{\\textnormal{running}}} = \\alpha \\sqrt{\\textnormal{Var}[x^k]_{\\textnormal{running}}} + (1-\\alpha) \\sqrt{\\textnormal{Var}[x^k]_{\\textnormal{batch}}}$$ where $\\alpha$ is a momentum term close to 1.\n",
    "\n",
    "Second, each normalized activation $\\hat{x}^k$ is then transformed by two trainable parameters, $\\gamma^k$ and $\\beta^k$. The final transformed activation value $h^k$ is given by $$h^k = \\gamma^k \\hat{x}^k + \\beta^k.$$ This second transformation ensures that BatchNorm can, in practice, learn the identity transformation and thus does not reduce the functional approximation capability of the neural network.\n",
    "\n",
    "Each transformation is differentiable, allowing BatchNorm layers to be incorporated into a gradient descent optimization algorithm.\n",
    "\n",
    "One way of intuitively understanding the purpose of the first transformation is in terms of preventing activation function saturation. Recall that the sigmoid activation function $1/(1+e^{-x})$ has a linear regime near $x=0$ where the gradient is close to $1$, but a saturated regime at large $|x|$ where the gradient is close to zero. If the magnitude of the activation value is very large, the magnitude of the gradient will be very small and gradient descent will take a long time to converge. By normalizing the activations to have mean 0 and variance 1, fewer units will be in the saturated regime and the network can train faster.\n",
    "\n",
    "The purpose of the second transformation is not immediately clear. After all, what is the purpose of multiplying by a variance and adding a mean right after subtracting off the mean and normalizing the variance? The explanation can be found by understanding the training dynamics of stacked transformations. Consider, for example, a linear network $$y = w_1 w_2 \\dots w_n x.$$ Because $x$ is multiplied only by linear matrices, $y$ can only represent linear transformations of $x$. However, when this network is trained during gradient descent, each of the weights are updated according to the gradient times the learning rate. As a result, the activation means can undergo large changes even after a small gradient update, depending on the values of $w_i$ in the below layers. This is called 'internal covariate shift'.  However, when the mean of each hidden unit is normalized using BatchNorm, the mean of each activation depends only on the parameter $\\beta^k$, and not on a complicated interaction between the gradient descent step size and the layer weights. As a result, the network activation means don't change dramatically in response to each gradient descent step size, and the network becomes easier to train.\n",
    "\n",
    "In practice, networks trained with BatchNorm can use larger learning rates than methods without BatchNorm. While training speed is not proportional to the learning rate used, empirically these larger learning rates lead to faster training speed. Furthermore, Batchnorm-optimized networks tend to have better performance than methods without using BatchNorm. \n",
    "\n",
    "While these intuitive explanations give the reader some sense of why BatchNorm might be successful, note that there is debate among researchers about why BatchNorm gives better performance in practice. Some researchers have found that BatchNorm doesn't actually reduce internal covariate shift, and that BatchNorm gives improved training speed only due to the larger learning rate allowed, while BatchNorm gives improved generalization due to the regularizing effects of stochastic gradient descent with larger step sizes.\n",
    "\n",
    "Good introductory resources on BatchNorm include section 8.7.1 of the [Deep Learning Book](https://www.deeplearningbook.org/) and the original [BatchNorm paper](https://arxiv.org/abs/1502.03167). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0020620-7db5-475b-b6df-83e2da2d6189",
   "metadata": {},
   "source": [
    "### 6.2.1: BatchNorm in Flax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7e267-e709-42a5-a594-2320ec72f390",
   "metadata": {},
   "source": [
    "Using a BatchNorm layer in Flax is also easy. `flax.nnx` only requires specifying explicitly the size of the activation map given as input to the BatchNorm layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d4cadc-742e-4634-b984-b698cab71a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormMLP(nnx.Module):\n",
    "    def __init__(self, din: int, dmid: int, dout: int, rngs: nnx.Rngs):\n",
    "        self.linear1 = nnx.Linear(din, dmid, rngs=rngs)\n",
    "        self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(dmid, dout, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.bn(self.linear1(x)))\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3da48134-e0cc-4f8c-905b-2b9aa771cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNormMLP(\n",
      "  linear1=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(3, 10), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(10,), dtype=float32)\n",
      "    ),\n",
      "    in_features=3,\n",
      "    out_features=10,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x10ca17920>,\n",
      "    bias_init=<function zeros at 0x10b4474c0>,\n",
      "    dot_general=<function dot_general at 0x10ac593a0>\n",
      "  ),\n",
      "  bn=BatchNorm(\n",
      "    mean=BatchStat(\n",
      "      value=Array(shape=(10,), dtype=float32)\n",
      "    ),\n",
      "    var=BatchStat(\n",
      "      value=Array(shape=(10,), dtype=float32)\n",
      "    ),\n",
      "    scale=Param(\n",
      "      value=Array(shape=(10,), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(10,), dtype=float32)\n",
      "    ),\n",
      "    num_features=10,\n",
      "    use_running_average=False,\n",
      "    axis=-1,\n",
      "    momentum=0.99,\n",
      "    epsilon=1e-05,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    use_bias=True,\n",
      "    use_scale=True,\n",
      "    bias_init=<function zeros at 0x10b4474c0>,\n",
      "    scale_init=<function ones at 0x10b4476a0>,\n",
      "    axis_name=None,\n",
      "    axis_index_groups=None,\n",
      "    use_fast_variance=True\n",
      "  ),\n",
      "  linear2=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(10, 2), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(2,), dtype=float32)\n",
      "    ),\n",
      "    in_features=10,\n",
      "    out_features=2,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x10ca17920>,\n",
      "    bias_init=<function zeros at 0x10b4474c0>,\n",
      "    dot_general=<function dot_general at 0x10ac593a0>\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BatchNormMLP(3, 10, 2, rngs=nnx.Rngs(0))\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090942e0-6e0f-4c99-8955-d3b85a17da4c",
   "metadata": {},
   "source": [
    "We can see that the BatchNorm layer has two variables of type `nnx.Param`: `scale` and `bias`. It also has two variables of type `nnx.BatchStat`: `mean` and `var`. The scale and bias correspond to the parameters $\\gamma$ and $\\beta$ from earlier, while the mean and variance correspond to running averages of the batch mean and variance calculated during training. Since there are 10 activations in the hidden layer, BatchNorm adds $2*10=20$ parameters to the model parameters.\n",
    "\n",
    "Note also that the BatchNorm layer begins in training mode, with `use_running_average=False`. When `model.eval()` is called, `use_running_average=True` and the model switches from updating the running averages `mean` and `var` to using them to normalize the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94d78242-a586-4a6f-a57a-eea2dd1e3f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(model.bn.use_running_average)\n",
    "model.eval()\n",
    "print(model.bn.use_running_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d670df-617a-4dce-90ed-94d2f77ad904",
   "metadata": {},
   "source": [
    "Note also that if dropout and BatchNorm are used simultaneously in the same layer, BatchNorm is usually applied first and Dropout is applied second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb8da04-caba-4d87-846c-07cd1eb89e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormMLP(nnx.Module):\n",
    "  def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):\n",
    "    self.linear1 = Linear(din, dmid, rngs=rngs)\n",
    "    self.dropout = nnx.Dropout(rate=0.1, rngs=rngs)\n",
    "    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n",
    "    self.linear2 = Linear(dmid, dout, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    x = nnx.relu(self.dropout(self.bn(self.linear1(x)))) # BatchNorm applied first, Dropout second\n",
    "    return self.linear2(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
