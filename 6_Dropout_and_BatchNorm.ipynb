{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474a95e3-894a-40f2-a937-808275196aa5",
   "metadata": {},
   "source": [
    "# Tutorial #6: Dropout and BatchNorm\n",
    "\n",
    "Dropout is a method of regularizing networks. Dropout is an extremely simple: during training, multiply the input to each unit by either 0 or 1, multiplying by 1 with probability $p$ and by 0 with probability $(1-p)$. Typically, $p=0.5$ is chosen for hidden units, and $p=0.8$ is chosen for input units. These have been shown to empirically work well. \n",
    "\n",
    "During testing, all of the units are retained; none are multipled by 0. Empiricially, it has been shown that dropout has better performance if the expected value of the output of each hidden unit matches the distribution seen during training. Since $1/p$ times more hidden units are included in each linear transformation during testing compared to training, the expected value of each output unit will be multiplied by $1/p$. To counteract this multiplication in expected value, the weights of the linear transformation are simply multiplied by $1/p$ during testing. \n",
    "\n",
    "For a layer within a neural network with input $\\boldsymbol{x}$, non-linear activation function $g(\\boldsymbol{z})$, and linear transformation matrix $\\boldsymbol{W}$, and bias $\\boldsymbol{b}$, the output $\\boldsymbol{h}$ of the hidden layer is given by $$\\boldsymbol{h} = g(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}).$$ Dropout performs elementwise multiplication of $\\boldsymbol{x}$ with a binary mask $\\boldsymbol{r}$ where each element of $\\boldsymbol{r}$ is drawn from a Bernoulli distribution with probability $p$. During training, the dropout layer becomes $$r_j \\sim \\textnormal{Bernoulli}(p)$$ $$ \\boldsymbol{\\tilde{x}} = \\boldsymbol{r} * \\boldsymbol{x}$$ $$\\boldsymbol{h} = g(\\boldsymbol{W} \\boldsymbol{\\tilde{x}} + \\boldsymbol{b}).$$ During testing, the layer now becomes $$\\boldsymbol{h} = g(\\frac{1}{p}\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}).$$\n",
    "\n",
    "TODO: is dropout still used in LLMs and state-of-the-art vision models?\n",
    "\n",
    "Good resources for understanding dropout include the original 2014 paper by [Srivastava et al.](TODO) and section 7.12 of the [Deep Learning Book](TODO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc3912-9265-4db4-abf2-2c4c004674fe",
   "metadata": {},
   "source": [
    "### 6.1: Dropout in `flax`\n",
    "\n",
    "Dropout in `flax` is extremely easy to implement. We simply create a `nnx.Dropout(p, rngs)` layer, and call it to mask the input to the each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca78324-4d2b-41a7-be66-a1206cbe2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import jax\n",
    "from flax import nnx\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a782c7c-6938-4a4d-8ec5-fd4571be66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutTestMLP(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.linear1 = nnx.Linear(4, 16, rngs=rngs)\n",
    "        self.dropout_in = nnx.Dropout(0.8, rngs = rngs)\n",
    "        self.linear2 = nnx.Linear(16, 10, rngs=rngs)\n",
    "        self.dropout = nnx.Dropout(0.5, rngs = rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.linear1(self.dropout_in(x)))\n",
    "        return self.linear2(self.dropout(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e9a6e-8768-4af1-a8ce-88b0e6670d23",
   "metadata": {},
   "source": [
    "We have to call `model.train()` to set the dropout layers to mask the inputs, and `model.eval()` to multiply the weights by $1/p$ for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159decb5-bcc6-4c01-bb48-1b2b4ad8e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train dropout, mask chosen randomly\n",
      "[-3.3543632   1.2700179  -6.3404975  -4.5124435   4.522447   -0.21097493\n",
      "  2.3068194   0.05415332  1.5272924   4.24723   ]\n",
      "Second train dropout, mask chosen randomly with different RNG key\n",
      "[-4.4009852  0.4407386 -3.4723148  1.7922347  7.803579  -2.2121768\n",
      " -1.5838387 -4.6009502 -2.6703286 -1.5528221]\n",
      "First eval dropout, no mask\n",
      "[-0.87284786  0.84970397 -1.2577161  -0.75616425  0.4537591  -0.6594648\n",
      "  0.9112347   0.19184317  0.8342736   1.534824  ]\n",
      "Second eval dropout, deterministic output same as first eval\n",
      "[-0.87284786  0.84970397 -1.2577161  -0.75616425  0.4537591  -0.6594648\n",
      "  0.9112347   0.19184317  0.8342736   1.534824  ]\n"
     ]
    }
   ],
   "source": [
    "model = DropoutTestMLP(nnx.Rngs(0))\n",
    "\n",
    "model.train()\n",
    "\n",
    "test_input = jax.random.normal(jax.random.PRNGKey(0), (4,))\n",
    "print(\"First train dropout, mask chosen randomly\")\n",
    "print(model(test_input))\n",
    "print(\"Second train dropout, mask chosen randomly with different RNG key\")\n",
    "print(model(test_input))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"First eval dropout, no mask\")\n",
    "print(model(test_input))\n",
    "print(\"Second eval dropout, deterministic output same as first eval\")\n",
    "print(model(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce946be-5114-4d62-b47e-c8304f1bc82c",
   "metadata": {},
   "source": [
    "When using Dropout layers, we have to remember to call `model.train()` before training and `model.eval()` before testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd384c8-925b-477e-a6b8-1312a706190c",
   "metadata": {},
   "source": [
    "### 6.2: BatchNorm in `flax`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
