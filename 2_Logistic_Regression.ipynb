{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7418ae83-65d8-4915-8460-a68949bb93e0",
   "metadata": {},
   "source": [
    "# Tutorial #2: Logistic Regression\n",
    "\n",
    "In this tutorial, we'll explain logistic regression and use it for classification of a few simple datasets. We'll begin by performing logistic regression analytically using a synthetic dataset for a single scalar variable and binary outcomes, then we'll use `scikit-learn` and `flax` to perform logistic regression on a multivariate dataset with binary outcomes.\n",
    "\n",
    "Assume that the output variable $y \\in {0, 1}$ is a binary variable, while the input variable $\\boldsymbol{x} \\in \\mathbb{R}$. The assumption in a linear regression model is that the output data $y=1$ with probability $\\theta$ and $y=0$ with probability $1-\\theta$. Thus, $$p(y|\\theta) = \\theta^y(1-\\theta)^{1-y}.$$ The probability $\\theta$ must be between 0 and 1. To ensure that $\\theta$ falls between 0 and 1, we set $\\theta = \\sigma(\\boldsymbol{z})$ where $\\sigma$ is the logistic function $$\\sigma(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1+e^{-z}}.$$ Note that $\\sigma(z \\rightarrow \\infty) = 1$ and $\\sigma(z \\rightarrow -\\infty) = 0$. While non-linear models can be used for $z$, the simplest assumption is that $z$ depends linearly on the inputs $\\boldsymbol{x}$, so that $z = \\boldsymbol{w}^T \\boldsymbol{x}$. This gives $$p(y|\\boldsymbol{x}, \\boldsymbol{w}) = \\sigma(\\boldsymbol{w}^T \\boldsymbol{x})^y(1-\\sigma(\\boldsymbol{w}^T \\boldsymbol{x}))^{1-y}.$$ We then maximize the log probability of the data, which is equivalent to minimizing a loss function given by the negative log probability of the data. For a dataset with $N$ examples $\\{\\boldsymbol{x}_{i}, {y}_i\\}_{i=1}^N$, the logistic regression loss function is thus $$\\boldsymbol{w} = \\arg \\min \\sum_{i=1}^N -y_i \\log{\\sigma(\\boldsymbol{w}^T\\boldsymbol{x}_i)} -(1-y_i)\\log{(1-{\\sigma(\\boldsymbol{w}^T\\boldsymbol{x})})}$$ We can generalize logistic regression from the binary classification case to the $K$-class classification case by replacing the Logistic function with the Softmax function $$\\sigma(\\boldsymbol{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "Useful references on Logistic Regression:\n",
    "* [COS324](https://www.cs.princeton.edu/courses/archive/spring19/cos324/) Lecture Notes, [Logistic Regression](https://www.cs.princeton.edu/courses/archive/spring19/cos324/files/logistic-regression.pdf)\n",
    "* [Probabilistic Machine Learning Book](https://probml.github.io/pml-book/book1.html) (Kevin Murphy), Chapter 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7cdfc1-bbd6-420d-898a-6babad7ffe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import jax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c07ac-1c68-4c0e-91c1-68397592cc13",
   "metadata": {},
   "source": [
    "### 2.1: Analytic Logistic Regression using `jax.numpy` and synthetic data\n",
    "\n",
    "We'll first generate some synthetic data. We'll have two classes which are not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d77cf8-800c-4e66-88f6-d3b812cf7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.key(0)\n",
    "N_data = 40\n",
    "\n",
    "def class_1(key, N):\n",
    "    return -2.5 + random.normal(key, (N,))\n",
    "\n",
    "def class_2(key, N):\n",
    "    return 0.5 + random.normal(key, (N,))\n",
    "\n",
    "key1, key2 = random.split(key)\n",
    "\n",
    "x_1 = class_1(key1, N_data//2)\n",
    "x_2 = class_2(key2, N_data//2)\n",
    "\n",
    "y_1 = jnp.zeros(x_1.shape)\n",
    "y_2 = jnp.ones(x_2.shape)\n",
    "\n",
    "plt.scatter(x_1, y_1, color='blue', marker='x')\n",
    "plt.scatter(x_2, y_2, color='red', marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507b7fa-1f16-42ee-8015-41303b57d3ad",
   "metadata": {},
   "source": [
    "We'll then assume that our data is given by a logistic regression model, with $p(y = 1 | x) = \\sigma(z)$ with $z = w x + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0431df3-f9e3-4aa5-abe1-aa64e61119f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jnp.concatenate([x_1, x_2])\n",
    "Y = jnp.concatenate([y_1, y_2])\n",
    "\n",
    "def logistic(z):\n",
    "    return 1 / (1 + jnp.exp(-z))\n",
    "\n",
    "def loss_function(x, y, w, b):\n",
    "    z = x * w + b\n",
    "    return -y * jnp.log(logistic(z)) - (1-y) * jnp.log(1 - logistic(z))\n",
    "\n",
    "loss_grad_fn = jax.value_and_grad(jax.jit(lambda w, b: jnp.mean(jax.vmap(loss_function, in_axes=(0,0,None,None), out_axes=0)(X, Y, w, b))), argnums=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df431ec-425f-457f-a26a-6dfb64ace06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "w_init = random.normal(subkey)\n",
    "b_init = 0.0\n",
    "N_train = 100000\n",
    "lr = 1e-3\n",
    "losses = []\n",
    "\n",
    "w = w_init\n",
    "b = b_init\n",
    "for _ in range(N_train):\n",
    "    loss, grads = loss_grad_fn(w, b)\n",
    "    losses.append(loss)\n",
    "    w = w - lr * grads[0]\n",
    "    b = b - lr * grads[1]\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b0010-2d6a-467a-a8fd-96d41a127025",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035079b9-1e7f-4af9-b5da-3eeb11e35627",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_1, y_1, color='blue', marker='x',label='class 0')\n",
    "plt.scatter(x_2, y_2, color='red', marker='x', label='class 1')\n",
    "x_plot = jnp.linspace(-5.0, 3.0, 1000)\n",
    "plt.plot(x_plot, logistic(x_plot * w + b), color='green',label='Logistic Classifier')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e50ef-fbc4-4feb-975f-88e18a4ba480",
   "metadata": {},
   "source": [
    "### 3.2: Scikit-learn for multivariate dataset\n",
    "\n",
    "We can read in the data (a breast cancer diagnostic dataset) into a pandas dataframe. We'll then separate the data into training and testing splits, with 80% of the data in the training set and 20% in the testing set. Since our data is made up of data 568 individuals, with no temporal state, we can split the training and testing data by sampling randomly without introducing data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1dd653-0f01-4588-90d9-9ba10383f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/breast_cancer_wisconsin/wdbc.data')\n",
    "dataset = df.to_numpy()\n",
    "X = jnp.asarray(dataset[:,2:].astype(float))\n",
    "y = jnp.asarray((dataset[:,1] == 'B').astype(int))\n",
    "\n",
    "key = random.key(0)\n",
    "key, subkey = random.split(key)\n",
    "X_shuffled = random.permutation(subkey, X, axis=0)\n",
    "y_shuffled = random.permutation(subkey, y, axis=0)\n",
    "\n",
    "N_div = int(0.8 * X.shape[0])\n",
    "X_train = X_shuffled[:N_div]\n",
    "y_train = y_shuffled[:N_div]\n",
    "X_test = X_shuffled[N_div:]\n",
    "y_test = y_shuffled[N_div:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b9822-a14f-4cca-9517-0403b9fee8de",
   "metadata": {},
   "source": [
    "Next we can use sklearn to perform logistic regression. With the `.fit()` function we can train a model, and with the `.score()` function we can evaluate performance on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678903a0-df83-41ef-8439-815d940b9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty='l2', max_iter=10000)\n",
    "trained_model = model.fit(X_train, y_train)\n",
    "\n",
    "print(trained_model.coef_)\n",
    "print(trained_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d035a62-a464-451f-8413-d2471bfe6878",
   "metadata": {},
   "source": [
    "By the way, we could have also imported the dataset from the `sklearn` library, using `from sklearn.datasets import load_breast_cancer` and\n",
    "`data = load_breast_cancer()` with `X = data.data` and `y = data.target`.\n",
    "\n",
    "We'll now evaluate model performance using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a4622-0496-43a6-9aee-84cb18b24e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be638f-c50d-4100-abd2-22848d9e4d61",
   "metadata": {},
   "source": [
    "We get about 93.8% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41a6f1-974a-4ddb-bd79-87c0909e6599",
   "metadata": {},
   "source": [
    "### 3.3: `flax` for multivariate binary classification with logistic regression\n",
    "\n",
    "We'll first write a logistic regression model in `flax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554e0aa-4bb7-4cd3-8828-40a5e489072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "\n",
    "class LogisticRegression(nnx.Module):\n",
    "    def __init__(self, din: int, rngs: nnx.Rngs):\n",
    "        self.linear = nnx.Linear(din, 1, rngs=rngs)\n",
    "\n",
    "    def logistic(z):\n",
    "        return 1 / (1 + jnp.exp(-z))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return logistic(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cef0e3-b411-4895-b6ca-3299033df0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "model = LogisticRegression(X.shape[1], rngs=rngs)\n",
    "optimizer = nnx.Optimizer(model, optax.sgd(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42c363-ae8c-490f-a53b-2faf04a441a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, X_train, y_train):\n",
    "    def loss_fn(model):\n",
    "        theta = nnx.vmap(model)(X_train)[:,0]\n",
    "        print(theta.shape)\n",
    "        print(y_train.shape)\n",
    "        return jnp.mean(- y_train * jnp.log(theta) - (1 - y_train) * jnp.log(1 - theta))\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa99a3-7480-4ce7-9483-95a1aafb3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_step(model, optimizer, X_train, y_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a427b-c634-456d-af5d-0839be0ce702",
   "metadata": {},
   "source": [
    "Oh no! Our loss function is giving us `NaN`s. The culprit is the `logistic` function, which gives `NaN` for large values of `z`. We'll have to find a way to eliminate `NaN`s from the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d8114-ca11-4ee5-8cb9-ac0ab5ea8fc0",
   "metadata": {},
   "source": [
    "#### 3.3.1: Normalize data\n",
    "\n",
    "While we could use clever numerical tricks to prevent `Nan` and `inf` within the loss function, a simpler approach would be to normalize the dataset. For each feature (in both the training and testing sets), we'll subtract by the mean of the training data and divide by the standard deviation of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbc57b-c115-4e74-9e3a-c77b1b117855",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = jnp.mean(X_train, axis=0)\n",
    "stds = jnp.std(X_train, axis=0)\n",
    "\n",
    "X_train_normalized = (X_train - means) / stds\n",
    "X_test_normalized = (X_test - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455343c1-fdad-4566-8bd4-f901ed9c3868",
   "metadata": {},
   "source": [
    "Now let's try computing the loss function with the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e03505-aab3-48e5-a5e3-9657541daed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "model = LogisticRegression(X.shape[1], rngs=rngs)\n",
    "optimizer = nnx.Optimizer(model, optax.sgd(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e8e06-b09a-4cea-be09-7950d81bfd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_step(model, optimizer, X_train_normalized, y_train)\n",
    "print(loss)\n",
    "loss = train_step(model, optimizer, X_train_normalized, y_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28c2d3-466c-4051-9b04-f975969ecaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "N_train = 50000\n",
    "for _ in range(N_train):\n",
    "    loss = train_step(model, optimizer, X_train_normalized, y_train)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec577ff-4a29-44ec-8249-bc80ccd5a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4c78a-3287-4007-9579-c5a95329e9ba",
   "metadata": {},
   "source": [
    "We'll now evaluate the performance of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add67f1-bcbf-4102-9964-9e868686b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test = nnx.vmap(model)(X_test_normalized)[:,0]\n",
    "y_test_eval = (theta_test > 0.5).astype(int)\n",
    "accuracy = jnp.mean((y_test == y_test_eval).astype(int))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5859849-30f7-4596-9d28-7d6c8b886ddf",
   "metadata": {},
   "source": [
    "We get 96.5% accuracy on the test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
